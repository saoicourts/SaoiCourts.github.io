<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Research Overview</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/beige.css" id="theme">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css" id="highlight-theme">

		<style>
			.blue-highlight { color: #1b91ff}
			.red-highlight {color: crimson}
			.green-highlight {color:rgb(127, 167, 60)}
			.orange-highlight {color:darkorange}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					<textarea data-template>
						# Research Overview
						### Nico Courts
						#### UW Seattle & Pacific Northwest National Laboratory
						#### nico@nicocourts.com
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						Slides for this presentation can be found at
						#### https://nicocourts.com/talks/research-talk/
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Outline
						- Representation theory
							- General ideas, Hopf algebras & BQCIs, support
						- Data Science & machine learning
							- Fuzzy simplicial complexes
								- Foundations, orthogonal label sets, FSN
							- Fiber bundle networks <!--.element: class="blue-highlight"-->
								- Foundations, BundleNets, applications & future directions
					</textarea>
				</section>
				<section>
					<h2>Part I: Representation Theory</h2>
				</section>
				<!--<section data-markdown data-auto-animate>
					<textarea data-template>
						### What is rep. theory?
						- The study of symmetries
						- The practice of studying things through *linearization*
						- The study of how things (e.g. groups) act
						- The study of $R$-$\operatorname{mod}$ for a ring $R$
					</textarea>
				</section>-->
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### What is rep. theory?
						- The study of symmetries
						- The practice of studying things through *linearization*
						- The study of how things (e.g. groups) act
							- <span class='blue-highlight'>"Study objects by examining what they **do**"</span>
						- The study of $R$-$\operatorname{mod}$ for a ring $R$
							- <span class='green-highlight'>What is the structure of this category?</span>
					</textarea>
				</section>
				<section>
					<section>
						<h3>Hopf algebras</h3>
						<p>The primary object of study for us will be <span class="blue-highlight">Hopf algebras</span>, which
						are a family of algebras with rich structure satisfying some symmetric properties.</p>
						<figure>
							<img src="images/co-monoid.jpg">
							<figcaption>
								<span style="font-size:15px">Monoid and Comonoid diagrams</span>
							</figcaption>
						</figure>
					</section>
					<section>
						<h3>Further properties</h3>
						<ul>
							<li>Existence of inverse/antipode $S:M\to M$</li>
							<li>Compatibility of $\mu,$ $\Delta,$ and $S$</li>
						</ul>
						<figure>
							<img width=80% src='images/bialgebra.svg'>
							<figcaption>
								<span style="font-size:15px">Compatibility of multiplication and comultiplication (from <a href="https://en.wikipedia.org/wiki/Bialgebra">https://en.wikipedia.org/wiki/Bialgebra</a>)</span>
							</figcaption>
						</figure>
					</section>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Bosonized quantum complete intersections
						In our work, we look at algebras within a particular class. We can heuristically identify
						them by adding adjectives.
						- Complete intersection rings of the form $$A=k[x_1,\dots,x_n]/(x_1^p,\dots,x_n^p)$$
						- <span class="blue-highlight">Quantum</span> complete intersection rings $$A_q=k_q[x_1,\dots,x_n]/(x_1^p,\dots,x_n^p)$$
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Bosonized quantum complete intersections
						In our work, we look at algebras within a particular class. We can heuristically identify
						them by adding adjectives.
						- <span class="blue-highlight">Quantum</span> complete intersection rings $$A_q=k_q[x_1,\dots,x_n]/(x_1^p,\dots,x_n^p)$$
						- <span class="green-highlight">Bosonized</span> <span class="blue-highlight">quantum</span> complete intersection rings $$\Lambda = A_q\rtimes(\mathbb{Z}/p\mathbb{Z})^n$$
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Support
						The category $R\text{-}\operatorname{mod}$ can get quite unruly when $R$ isn't very nice.
						
						Even relatively simple rings (e.g. $kG$ where $k=\overline{\mathbb{F}_3}$ and $G=\mathbb{Z}/3\mathbb{Z}\times\mathbb{Z}/3\mathbb{Z}$) are 
						of what we call "<span class="red-highlight">wild type</span>."
						
						So we introduce a notion of <span class="green-highlight">support</span>, which gives us a way to treat a category like a ring and define its spectrum.
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Our work
						Our current focus is to show the following notions of support coincide:
						- <span class="blue-highlight">Cohomological support variety</span> &ndash; $\operatorname{Proj}(H^\bullet(\Lambda, k))$
						- <span class="red-highlight">Hypersurface support variety</span> &ndash; looking at "places" where modules have infinite cohomological dimension
						- <span class="green-highlight">Rank variety</span> &ndash; looking at places certain matrices have maximal rank.
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Our work
						This is all in service of proving an incarnation of the <span class="orange-highlight">tensor product property</span> of support, which states, for all modules $M$ and $N$
						$$\operatorname{supp}(M\otimes N)=\operatorname{supp}(M)\cap\operatorname{supp}(N)$$
					</textarea>
				</section>

				
				
				<section>
					<h2>Part II: Fuzzy Simplicial Complexes</h2>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Background and overview
						During the summer of 2020 I was accepted to participate in an internship at <span class="blue-highlight">Pacific Northwest National Lab</span> (PNNL) on the <span class="orange-highlight">Data Science & Analytics</span> (DSA) team.

						I was assigned to a real project with a real client and made substantial contributions that resulted in a publication.

						At the end of the summer, I was <span class="green-highlight">invited to stay on part-time</span> while I finished my studies.
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### The context: Fewshot learning 
						The use case that was most useful to our client was an image classification model that could make classifications on <span class="red-highlight">previously-unseen data</span>.

						<span class="blue-highlight">Fewshot learning</span> is a machine learning paradigm where the model is trained in *episodes.* During each episode, the model is asked to classify images given some small (usually $\le 10$) examples of each class.
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### The context: "CV generalist" 
						In all our work on this, we train a base model as a <span class="orange-highlight">computer vision generalist</span>&mdash;that is, exposed to a large, general-purpose data set without taking
						any gradient steps on our evaluation set. The idea is to train one model <span class="green-highlight">which can be reused</span> for any image classification task that may arise.
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### What's happening?
						We created a UMAP projection of the SNS dataset (irrespective of labels) then colored the data according to the Fruits 360 labels (left) and Stem/No-Stem labels (right) to visualize the pathology.

						![our paper](images/SNS.png)<!--.element: style="width:70%"-->
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### The problem: Orthogonal labels
						Our team identified a failure mode of fewshot models wherein they perform well on datasets with a particular label set, but fail when an "orthogonal" label set is used.
						![stem/no-stem](images/orthogonal-labels.png)<!--.element: style="width:80%"-->

						<span style="font-size:15px">Examples from our three constructed datasets: Stem/No-Stem, Back/No-Back, One/Many</span>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Our solution
						Our model is in the family of metric-based models that define a (pseudo)metric in feature space in order to use distances to determine class membership.

						In service of this, we model the manifold corresponding to each class using a <span class="green-highlight">fuzzy simplicial complex.</span> These are generalizations
						of simplicial complexes
						- <span class="orange-highlight">that are not necessarily downward-closed</span>
						- <span class="blue-highlight">in which each simplex is assigned a continuous (instead of binary) membership value</span>.
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Results
						![our results](images/SNS_results.png)<!--.element: style="width:70%"-->
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Results
						![our results](images/fsn_results.png)<!--.element: style="width:70%"-->
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Our paper
						Our work on FSN resulted in a paper that was published in a workshop at AAAI 2021:

						![our paper](images/fsn_paper.png)<!--.element: style="width:70%"-->
					</textarea>
				</section>



				<section>
					<h2>Part III: Bundle Networks</h2>
					<h3><span class='green-highlight'>Problem formulation and approach</span></h3>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Background and overview
						Work completed during my second summer at PNNL.

						Resulting work was written up into [a manuscript](https://arxiv.org/abs/2110.06983) that has been submitted to ICLR 2022 (with generally favorable reviews so far).

						Will result in a <span class="green-highlight">public GitHub repo</span> with data and code for reproducibility once it is cleared by the lab for release.
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Motivation: Materials science
						During this portion of my internship, I served alongside data scientists who were helping materials scientists analyze their fabrication data. 

						An early observation we made is that the manufacturing process naturally formed a <span class="red-highlight">many-to-one map.</span>
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Motivation: Materials science
						There are many human-designed parameters that can be changed to affect the end product, but often researchers are only interested 
						in one (or very few) output parameters.

						![many-to-one process](images/many-to-one.png)<!--.element: style="width:30%"-->

						One question one may ask is <span class="green-highlight">"In what ways can I change my configuration that keeps the end product constant?"</span>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### A mathematical perspective
						Whenever we, as mathematicians, encounter many-to-one maps, we begin to wonder what kinds of structure such a map may have. Common examples of 
						surjective maps in topology and geometry include
						- (Topological) quotients
						- <span class="blue-highlight">Fiber Bundles</span>
						- Covering maps 
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Idea: Fiber bundles
						![bundles](images/bundle.png)<!--.element: style="width:60%"-->

						A <span class="blue-highlight">fiber bundle</span> is a base space $X$, a total space $E$, and a surjection $\pi:E\to X$ satisfying <span class="red-highlight">local triviality.</span>
						In essence, this means that $E$ is a space parameterized by $X$ that is <span class="green-highlight">basically the same everywhere.</span>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Why fiber bundles?
						<span class="red-highlight">Quotient maps</span> seemed too general a framework for our case. 
						
						We worked under the hypothesis that the modes of variance that were collapsed 
						over each point were comparable. <span class="blue-highlight">Fiber bundles</span> were the right blend of generality and structure for this assumption.

						<span class="green-highlight">Covering maps</span> seemed too specific (they are fiber bundles with discrete fibers).
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Distilling the problem
						Under the assumption that the true data lies on a fiber bundle, we can rephrase the problem in the following way:

						Create a model that performs density estimation on the <span class="orange-highlight">fiber over a point</span>, that is, $\pi^{-1}(y)$ for any $y$ in the base space. 
						In doing so, construct a <span class="red-highlight">fiberwise-accurate reconstruction</span> of the true bundle underlying the data.
					</textarea>
				</section>
				<section>
					<h2>Part III: Bundle Networks</h2>
					<h3><span class='orange-highlight'>Model selection &amp; tuning</span></h3>
				</section>
				<section data-markdown>
					<textarea data-template>
						#### Capabilities required
						- Sample from the distribution of points on the fiber over a point
						- Use a single model (instead of an ensemble)

						#### Expected difficulties <!--.element: style="margin-top:30px"-->
						- Data will be sparse (finite) in general
						- Global data distribution may be too complicated for traditional architectures to model
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Overview
						We use <span class="orange-highlight">fiber bundles</span> to address the fact we expect the local data distribution is 
						simpler than the global and that local structure will be somewhat homogeneous.

						We use <span class="green-highlight">normalizing flows</span> to perform density estimation in trivial neighborhoods.

						We use <span class="red-highlight">invertible neural nets</span> to implement our model, allowing simultaneous forward and backward
						training, as well as lossless connection between $\pi^{-1}(U)$ and $U\times F$.
					</textarea>
				</section>
				<section>
					<section>
						<h3>Density estimation using normalizing flows</h3>
						<p>
							In (<a href="http://proceedings.mlr.press/v37/rezende15.html">Rezende and Mohammed, 2015</a>), 
							the authors suggest an approach to density estimation (i.e. posterior approximation) 
							using the framework of normalizing flows. 
						</p><p>
							Here, given some choice of prior $p$ on space $X$, 
							we learn a function $\Phi:Y\to X$ which gives distribution $q$ on $Y$ <span class="green-highlight">parameterized by $p$</span>:
						</p>
						$$q(y')=p(\Phi(y'))|D\Phi(y')|$$
					</section>
					<section>
						<h4>References</h4>
						<span style="font-size:25px"><strong>Danilo Rezende, Shakir Mohamed</strong> "Variational Inference with Normalizing Flows" <em>Proceedings of the 32nd International Conference on Machine Learning</em>, PMLR 37:1530-1538, 2015.
						(<a href="http://proceedings.mlr.press/v37/rezende15.html">link</a>)</span>
					</section>
				</section>
				<section>
					<section>
						<h3>Invertible neural nets</h3>
						<p>
							INNs are a class of NN blocks that are, by construction, invertible. They were first used by <a href="https://arxiv.org/abs/1410.8516">(Dinh et al., 2015)</a> to perform a 
							kind of ICA and then in <a href="https://arxiv.org/abs/2001.04872">(Dinh et al., 2017)</a> to perform density estimation.
						</p>
						<p>
							The work in <a href="https://arxiv.org/abs/2001.04872">(Sorrenson et al., 2019)</a> served as an inspiration for the power of this approach 
							in their <span class="blue-highlight">automated feature discovery and isolation</span> applied to MNIST.
						</p>
					</section>
					<section>
						<h4>References</h4>
						<span style="font-size:25px">
							<strong>Laurent Dinh, David Krueger, and Yoshua Bengio</strong> 
							"NICE: Non-linear Independent Components Estimation" 
							<em>arXiv preprint arXiv:1410.8516</em>, 2015
							(<a href="https://arxiv.org/abs/1410.8516">link</a>)
						</span><br /><br />

						<span style="font-size:25px">
							<strong>Peter Sorrenson, Carsten Rother, and Ullrich Köthe</strong>
							"Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)" 
							<em>International Conference on Learning Representations</em>, 2019
							(<a href="https://arxiv.org/abs/2001.04872">link</a>)
						</span><br /><br />

						<span style="font-size:25px">
							<strong>Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio</strong>
							"Density estimation using real NVP"
							<em>International Conference on Learning Representations</em>, 2017
							(<a href="https://arxiv.org/abs/2001.04872">link</a>)
						</span>
					</section>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Model architecture
						**Input:** Data in the total space $\mathcal{D}_\text{bundle}\subseteq X$ and base space $\mathcal{D}_\text{base}\subseteq Y$.
						- Compute <span class="blue-highlight">neighborhoods $\\{U_i\\}_{i\in\mathcal I}$</span> in $\mathcal{D}_\text{base}$, 
						each with a <span class="red-highlight">representative point $r_i\in U_i$</span>.
						- To run the model, <span class="red-highlight">condition on nearest $r\in \mathcal{R}=\\{r_i\\}_{i\in\mathcal I}$</span>.
						- Loss encourages <span class="blue-highlight">bundle data to lie in the proper fiber</span> as well as <span class="green-highlight">general shape and density</span>
						of points in each neighborhood.
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						### Model architecture
						![model](images/model.png)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Notation
						In the following we use
						- $\Phi:X\times \mathcal R\to Y\times \mathcal D$ is our model
							- $\Phi_i=\Phi(-,r_i):X\to Y\times\mathcal D$

						- $x$ is a point in $\mathcal{D}_\text{bundle}$ lying over $y\in\mathcal{D}_\text{base}$.
						- $r_i$ is $\operatorname{argmin}_\{r_j\in\mathcal R\}\\|y-r_j\\|$
						- $p_Z$ is projection onto the $Z$ component
						- Sample $z_k\sim\mathcal D$ and let $\widetilde X=\\{\Phi_i^{-1}(y, z_k)\\}$
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Loss terms
						- Forward loss
							- **Mean squared error**: $\\|y-p_Y(\Phi(x))\\|^2$
						- Backward loss 
							- **KL-fwd**: $D_\text{KL}(\pi^\{-1\}(y)||\widetilde X)$
							- **KL-bwd**: $D_\text{KL}(\widetilde X||\pi^\{-1\}(y))$
							- **Mean squared minimum distance (MSMD)**:
							$$\frac{1}{|\widetilde X|}\sum_{\tilde x\in\widetilde X}\min_{x\in\pi^{-1}(y)}\\|x-\tilde x\\|$$
					</textarea>
				</section>
				<section>
					<h2>Part III: Bundle Networks</h2>
					<h3><span class='red-highlight'>Performance &amp; analysis</span></h3>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Models tested
						- Our model, <span class="blue-highlight">BundleNet</span>
						- <span class="green-highlight">WGAN</span>&mdash;Wasserstein GAN 
						- <span class="red-highlight">CGAN</span>&mdash;Conditional GAN 
							- Conditioned on individual base points 
							- Batches drawn from global distribution
						- <span class="orange-highlight">CGAN-local</span>&mdash;A CGAN that
							- is conditioned on finitely many neighborhood representatives
							- is trained with batches drawn from a single neighborhood at a time
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Datasets
						- <span class="blue-highlight">Torus</span>, points drawn from a torus in $\mathbb R^3$ lying over a circle.
						- <span class="green-highlight">Möbius Band</span>, points drawn from a Möbius band in $\mathbb R^3$ lying over a circle.
						- <span class="orange-highlight">Wine Quality</span>, a real dataset consisting of 11 measured quantities, with each being assigned a (color, quality) pair.
						- <span class="red-highlight">Airfoil Noise</span>, a real dataset consisting of five parameters and their measured effect on sound pressure.
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Results
						![bundlenet results](images/bn-results.png)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Reconstructions
						<div style="display:flex; justify-content: center;"><div id='bn-torus' style="width:50%"></div><div id='cgan-torus' style="width:50%"></div></div>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Reconstructions
						<div style="display:flex; justify-content: center;"><div id='bn-moeb' style="width:50%"></div><div id='cgan-moeb' style="width:50%"></div></div>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Observation: Circular vs Gaussian priors
						![priors](images/prior_pictures.png)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Observation: Circular priors may work better?
						![priors](images/priors.png)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Observation: Fibers need not be uniform
						<div id='bn-sliced'></div>
					</textarea>
				</section>
				


				<section>
					<h2>Thank you!</h2>
				</section>
			</div>
		</div>

		<script src="../dist/reveal.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/zoom/zoom.js"></script>
		<script src="https://d3js.org/d3.v7.min.js"></script>
		<script src="https://cdn.plot.ly/plotly-2.6.3.min.js"></script>


		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealMath, RevealZoom ]
			});

			// helper function for plotly
			function unpack(rows, key) {
				return rows.map(function(row) { return row[key]; });
			}

			// Load data and populate graph
			fetch('data/BN_torus.json')
				.then(response => response.json())
				.then(function(data){
					var unpacked = [{
						x: unpack(data, 'bundle_x'),
						y: unpack(data, 'bundle_y'),
						z: unpack(data, 'bundle_z'),
						mode: 'markers',
						type: 'scatter3d',
						
						marker: {
						color: unpack(data, 'angle'),
						size: 2,
						colorscale: 'Jet',
						}
					}];

					layout = {
						title: "BundleNet Torus Reconstruction",
						scene:{
							aspectmode: "manual",
							aspectratio: {
							x: 1, y: 1, z: 1,
							},
							xaxis: {
							nticks: 9,
							range: [-10, 10],
							},
							yaxis: {
							nticks: 7,
							range: [-10, 10],
							},
							zaxis: {
							nticks: 10,
							range: [-10, 10],
							}
						}
					};

					// plot!
					Plotly.newPlot('bn-torus', unpacked, layout);
				})
				// CGAN torus
				fetch('data/CGAN_triv_torus.json')
				.then(response => response.json())
				.then(function(data){
					var unpacked = [{
						x: unpack(data, 'bundle_x'),
						y: unpack(data, 'bundle_y'),
						z: unpack(data, 'bundle_z'),
						mode: 'markers',
						type: 'scatter3d',
						
						marker: {
						color: unpack(data, 'angle'),
						size: 2,
						colorscale: 'Jet',
						}
					}];

					layout = {
						title: "CGAN-local Torus Reconstruction",
						scene:{
							aspectmode: "manual",
							aspectratio: {
							x: 1, y: 1, z: 1,
							},
							xaxis: {
							nticks: 9,
							range: [-10, 10],
							},
							yaxis: {
							nticks: 7,
							range: [-10, 10],
							},
							zaxis: {
							nticks: 10,
							range: [-10, 10],
							}
						}
					};

					// plot!
					Plotly.newPlot('cgan-torus', unpacked, layout);
				})
			// moeb
			fetch('data/BN_moebius.json')
				.then(response => response.json())
				.then(function(data){
					var unpacked = [{
						x: unpack(data, 'bundle_x'),
						y: unpack(data, 'bundle_y'),
						z: unpack(data, 'bundle_z'),
						mode: 'markers',
						type: 'scatter3d',
						
						marker: {
						color: unpack(data, 'angle'),
						size: 2,
						colorscale: 'Jet',
						}
					}];

					layout = {
						title: "BundleNet Moebius Band Reconstruction",
						scene:{
							aspectmode: "manual",
							aspectratio: {
							x: 1, y: 1, z: 1,
							},
							xaxis: {
							nticks: 9,
							range: [-10, 10],
							},
							yaxis: {
							nticks: 7,
							range: [-10, 10],
							},
							zaxis: {
							nticks: 10,
							range: [-10, 10],
							}
						}
					};

					// plot!
					Plotly.newPlot('bn-moeb', unpacked, layout);
				})
				// CGAN moeb
				fetch('data/CGAN_triv_moebius.json')
				.then(response => response.json())
				.then(function(data){
					var unpacked = [{
						x: unpack(data, 'bundle_x'),
						y: unpack(data, 'bundle_y'),
						z: unpack(data, 'bundle_z'),
						mode: 'markers',
						type: 'scatter3d',
						
						marker: {
						color: unpack(data, 'angle'),
						size: 2,
						colorscale: 'Jet',
						}
					}];

					layout = {
						title: "CGAN-local Moebius Band Reconstruction",
						scene:{
							aspectmode: "manual",
							aspectratio: {
							x: 1, y: 1, z: 1,
							},
							xaxis: {
							nticks: 9,
							range: [-10, 10],
							},
							yaxis: {
							nticks: 7,
							range: [-10, 10],
							},
							zaxis: {
							nticks: 10,
							range: [-10, 10],
							}
						}
					};

					// plot!
					Plotly.newPlot('cgan-moeb', unpacked, layout);
				})
			// sliced
			fetch('data/BN_sliced.json')
				.then(response => response.json())
				.then(function(data){
					var unpacked = [{
						x: unpack(data, 'bundle_x'),
						y: unpack(data, 'bundle_y'),
						z: unpack(data, 'bundle_z'),
						mode: 'markers',
						type: 'scatter3d',
						
						marker: {
						color: unpack(data, 'angle'),
						size: 2,
						colorscale: 'Jet',
						}
					}];

					layout = {
						title: "BundleNet Sliced Torus Reconstruction",
						scene:{
							aspectmode: "manual",
							aspectratio: {
							x: 1, y: 1, z: 1,
							},
							xaxis: {
							nticks: 9,
							range: [-10, 10],
							},
							yaxis: {
							nticks: 7,
							range: [-10, 10],
							},
							zaxis: {
							nticks: 10,
							range: [-10, 10],
							}
						}
					};

					// plot!
					Plotly.newPlot('bn-sliced', unpacked, layout);
				})
		</script>
	</body>
</html>
